{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "34b1d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f4bc5ec",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium\n",
      "  Obtaining dependency information for gymnasium from https://files.pythonhosted.org/packages/a8/4d/3cbfd81ed84db450dbe73a89afcd8bc405273918415649ac6683356afe92/gymnasium-0.29.1-py3-none-any.whl.metadata\n",
      "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\tessa\\anaconda3\\lib\\site-packages (from gymnasium) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\tessa\\anaconda3\\lib\\site-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\tessa\\anaconda3\\lib\\site-packages (from gymnasium) (4.7.1)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Obtaining dependency information for farama-notifications>=0.0.1 from https://files.pythonhosted.org/packages/05/2c/ffc08c54c05cdce6fbed2aeebc46348dbe180c6d2c541c7af7ba0aa5f5f8/Farama_Notifications-0.0.4-py3-none-any.whl.metadata\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
      "   ---------------------------------------- 0.0/953.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 10.2/953.9 kB ? eta -:--:--\n",
      "   - ------------------------------------- 41.0/953.9 kB 667.8 kB/s eta 0:00:02\n",
      "   --- ------------------------------------ 92.2/953.9 kB 1.1 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 163.8/953.9 kB 1.2 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 225.3/953.9 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 286.7/953.9 kB 1.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 337.9/953.9 kB 1.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 409.6/953.9 kB 1.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 471.0/953.9 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 532.5/953.9 kB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 583.7/953.9 kB 1.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 665.6/953.9 kB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 727.0/953.9 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 778.2/953.9 kB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 839.7/953.9 kB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 880.6/953.9 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 953.9/953.9 kB 1.3 MB/s eta 0:00:00\n",
      "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, gymnasium\n",
      "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "334af3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "06549602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 5., 5., 5.], dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()[0]\n",
    "for i in range(4):\n",
    "    state[i] = np.digitize(state[i],state_dimensions[i])\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba968cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "931374e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e4d1894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_play():\n",
    "    for episode in range(10):\n",
    "        env.reset()\n",
    "        avg_reward = 0\n",
    "        n=0\n",
    "        done = False\n",
    "        while not done:\n",
    "            n+=1\n",
    "            env.render()\n",
    "            \n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "            state,reward,terminated,truncated,info = env.step(action)\n",
    "            avg_reward += (reward)\n",
    "            \n",
    "            done = terminated or truncated\n",
    "            \n",
    "        print(\"average reward is: \")\n",
    "        print(avg_reward)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "78301795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "43e6a280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "731623eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "01ced258",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "773817cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0, 0,0.09998\n",
      "16.73, 1000,0.07997999999999388\n",
      "30.27, 2000,0.05997999999998863\n",
      "78.94, 3000,0.03997999999998945\n",
      "82.82, 4000,0.019979999999990262\n",
      "91.0, 5000,0\n",
      "86.65, 6000,0\n",
      "90.53, 7000,0\n",
      "92.84, 8000,0\n",
      "91.69, 9000,0\n",
      "91.23, 10000,0\n",
      "93.35, 11000,0\n",
      "94.96, 12000,0\n",
      "96.67, 13000,0\n",
      "99.12, 14000,0\n",
      "94.99, 15000,0\n",
      "98.13, 16000,0\n",
      "97.43, 17000,0\n",
      "94.29, 18000,0\n",
      "98.16, 19000,0\n",
      "96.66, 20000,0\n",
      "97.68, 21000,0\n",
      "98.77, 22000,0\n",
      "95.99, 23000,0\n",
      "102.02, 24000,0\n",
      "96.17, 25000,0\n",
      "95.91, 26000,0\n",
      "95.03, 27000,0\n",
      "97.72, 28000,0\n",
      "99.95, 29000,0\n",
      "94.79, 30000,0\n",
      "96.8, 31000,0\n",
      "95.07, 32000,0\n",
      "99.46, 33000,0\n",
      "99.15, 34000,0\n",
      "95.36, 35000,0\n",
      "98.22, 36000,0\n",
      "98.74, 37000,0\n",
      "98.06, 38000,0\n",
      "99.85, 39000,0\n",
      "96.4, 40000,0\n",
      "98.45, 41000,0\n",
      "95.69, 42000,0\n",
      "96.5, 43000,0\n",
      "99.08, 44000,0\n",
      "97.18, 45000,0\n",
      "95.65, 46000,0\n",
      "95.64, 47000,0\n",
      "96.11, 48000,0\n",
      "98.26, 49000,0\n",
      "95.67, 50000,0\n",
      "95.41, 51000,0\n",
      "94.4, 52000,0\n",
      "93.81, 53000,0\n",
      "98.58, 54000,0\n",
      "93.82, 55000,0\n",
      "98.94, 56000,0\n",
      "96.59, 57000,0\n",
      "96.79, 58000,0\n",
      "95.1, 59000,0\n",
      "98.73, 60000,0\n",
      "97.16, 61000,0\n",
      "94.86, 62000,0\n",
      "96.76, 63000,0\n",
      "97.25, 64000,0\n",
      "96.22, 65000,0\n",
      "98.53, 66000,0\n",
      "97.1, 67000,0\n",
      "97.19, 68000,0\n",
      "96.39, 69000,0\n",
      "97.19, 70000,0\n",
      "98.94, 71000,0\n",
      "96.75, 72000,0\n",
      "96.57, 73000,0\n",
      "96.42, 74000,0\n",
      "96.43, 75000,0\n",
      "95.2, 76000,0\n",
      "96.12, 77000,0\n",
      "98.45, 78000,0\n",
      "97.69, 79000,0\n",
      "97.82, 80000,0\n",
      "97.75, 81000,0\n",
      "94.56, 82000,0\n",
      "92.29, 83000,0\n",
      "94.44, 84000,0\n",
      "98.29, 85000,0\n",
      "96.39, 86000,0\n",
      "93.57, 87000,0\n",
      "97.32, 88000,0\n",
      "95.05, 89000,0\n",
      "97.62, 90000,0\n",
      "100.17, 91000,0\n",
      "99.19, 92000,0\n",
      "98.92, 93000,0\n",
      "97.99, 94000,0\n",
      "96.82, 95000,0\n",
      "97.97, 96000,0\n",
      "94.41, 97000,0\n",
      "97.95, 98000,0\n",
      "100.25, 99000,0\n"
     ]
    }
   ],
   "source": [
    "dimension = 10\n",
    "state_p = np.linspace(-4.8,+4.8,dimension)\n",
    "state_pv = np.linspace(-4,+4,dimension)\n",
    "state_a = np.linspace(-0.2095,+0.2095,dimension)\n",
    "state_av = np.linspace(-4,+4,dimension)\n",
    "\n",
    "state_dimensions = [state_p,state_pv,state_a,state_av]\n",
    "rewards_per_episode = []\n",
    "q_table = np.zeros((dimension+1,dimension+1,dimension+1,dimension+1,2))\n",
    "episodes = 100000\n",
    "e_dr = 2/episodes\n",
    "gamma = 0.99\n",
    "alpha = 0.1\n",
    "e = 0.1\n",
    "\n",
    "\n",
    "for episode in range(episodes):\n",
    "    rewards = 0\n",
    "    state = list(env.reset()[0])\n",
    "    done = False\n",
    "    \n",
    "    #digitizing\n",
    "    state_p = np.digitize(state[0], state_dimensions[0])\n",
    "    state_v = np.digitize(state[1], state_dimensions[1])\n",
    "    state_a = np.digitize(state[2], state_dimensions[2])\n",
    "    state_av = np.digitize(state[3], state_dimensions[3])\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        env.render()\n",
    "\n",
    "        #chosing action\n",
    "        \n",
    "        if np.random.rand() < e:\n",
    "            # Choose random action  (0=go left, 1=go right)\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state_p, state_v, state_a, state_av, :])\n",
    "\n",
    "        new_state,reward,terminated,_,_ = env.step(action)\n",
    "        new_state_p = np.digitize(new_state[0], state_dimensions[0])\n",
    "        new_state_v = np.digitize(new_state[1], state_dimensions[1])\n",
    "        new_state_a = np.digitize(new_state[2], state_dimensions[2])\n",
    "        new_state_av= np.digitize(new_state[3], state_dimensions[3])\n",
    "\n",
    "        q_table[state_p, state_v, state_a, state_av, action] = q_table[state_p, state_v, state_a, state_av, action] + alpha * (\n",
    "            reward + gamma*np.max(q_table[new_state_p, new_state_v, new_state_a, new_state_av,:]) - q_table[state_p, state_v, state_a, state_av, action]\n",
    "        )\n",
    "        state = new_state\n",
    "        state_p = new_state_p\n",
    "        state_v = new_state_v\n",
    "        state_a = new_state_a\n",
    "        state_av= new_state_av\n",
    "        rewards += reward\n",
    "\n",
    "        done = terminated or truncated\n",
    "    e = max((e-e_dr),0)    \n",
    "    rewards_per_episode.append(rewards)\n",
    "    mean_rewards = np.mean(rewards_per_episode[len(rewards_per_episode)-100:])\n",
    "        \n",
    "    if truncated:\n",
    "        print(\"truncated\")\n",
    "\n",
    "    \n",
    "    if episode%1000 == 0:\n",
    "        print(f\"{mean_rewards}, {episode},{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dcfa094c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]]\n",
      "\n",
      "\n",
      " [[[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]]\n",
      "\n",
      "\n",
      " [[[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]]\n",
      "\n",
      "\n",
      " [[[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]]\n",
      "\n",
      "\n",
      " [[[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]\n",
      "\n",
      "  [[0 0]\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   ...\n",
      "   [0 0]\n",
      "   [0 0]\n",
      "   [0 0]]]]\n"
     ]
    }
   ],
   "source": [
    "print(sum(q_table!=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1216b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q_table[state[0],state[1],state[2],state[3],action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8426d173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 11.0  Epsilon: 1.00  Mean Rewards 11.0\n",
      "Episode: 100 16.0  Epsilon: 1.00  Mean Rewards 22.7\n",
      "Episode: 200 16.0  Epsilon: 1.00  Mean Rewards 21.6\n",
      "Episode: 300 14.0  Epsilon: 1.00  Mean Rewards 23.7\n",
      "Episode: 400 17.0  Epsilon: 1.00  Mean Rewards 21.2\n",
      "Episode: 500 29.0  Epsilon: 1.00  Mean Rewards 23.3\n",
      "Episode: 600 23.0  Epsilon: 0.99  Mean Rewards 22.6\n",
      "Episode: 700 19.0  Epsilon: 0.99  Mean Rewards 23.1\n",
      "Episode: 800 18.0  Epsilon: 0.99  Mean Rewards 21.4\n",
      "Episode: 900 18.0  Epsilon: 0.99  Mean Rewards 24.7\n",
      "Episode: 1000 24.0  Epsilon: 0.99  Mean Rewards 22.4\n",
      "Episode: 1100 21.0  Epsilon: 0.99  Mean Rewards 23.6\n",
      "Episode: 1200 15.0  Epsilon: 0.99  Mean Rewards 21.9\n",
      "Episode: 1300 13.0  Epsilon: 0.99  Mean Rewards 22.9\n",
      "Episode: 1400 41.0  Epsilon: 0.99  Mean Rewards 20.5\n",
      "Episode: 1500 16.0  Epsilon: 0.99  Mean Rewards 22.1\n",
      "Episode: 1600 20.0  Epsilon: 0.98  Mean Rewards 24.2\n",
      "Episode: 1700 14.0  Epsilon: 0.98  Mean Rewards 23.8\n",
      "Episode: 1800 68.0  Epsilon: 0.98  Mean Rewards 22.9\n",
      "Episode: 1900 17.0  Epsilon: 0.98  Mean Rewards 21.7\n",
      "Episode: 2000 45.0  Epsilon: 0.98  Mean Rewards 22.4\n",
      "Episode: 2100 26.0  Epsilon: 0.98  Mean Rewards 24.9\n",
      "Episode: 2200 19.0  Epsilon: 0.98  Mean Rewards 24.8\n",
      "Episode: 2300 20.0  Epsilon: 0.98  Mean Rewards 22.5\n",
      "Episode: 2400 26.0  Epsilon: 0.98  Mean Rewards 23.2\n",
      "Episode: 2500 22.0  Epsilon: 0.98  Mean Rewards 23.4\n",
      "Episode: 2600 15.0  Epsilon: 0.97  Mean Rewards 24.0\n",
      "Episode: 2700 28.0  Epsilon: 0.97  Mean Rewards 23.1\n",
      "Episode: 2800 27.0  Epsilon: 0.97  Mean Rewards 24.0\n",
      "Episode: 2900 34.0  Epsilon: 0.97  Mean Rewards 24.6\n",
      "Episode: 3000 25.0  Epsilon: 0.97  Mean Rewards 24.3\n",
      "Episode: 3100 15.0  Epsilon: 0.97  Mean Rewards 23.3\n",
      "Episode: 3200 9.0  Epsilon: 0.97  Mean Rewards 25.1\n",
      "Episode: 3300 43.0  Epsilon: 0.97  Mean Rewards 22.4\n",
      "Episode: 3400 18.0  Epsilon: 0.97  Mean Rewards 25.6\n",
      "Episode: 3500 38.0  Epsilon: 0.97  Mean Rewards 24.0\n",
      "Episode: 3600 32.0  Epsilon: 0.96  Mean Rewards 21.1\n",
      "Episode: 3700 26.0  Epsilon: 0.96  Mean Rewards 21.6\n",
      "Episode: 3800 20.0  Epsilon: 0.96  Mean Rewards 26.8\n",
      "Episode: 3900 44.0  Epsilon: 0.96  Mean Rewards 24.4\n",
      "Episode: 4000 15.0  Epsilon: 0.96  Mean Rewards 24.1\n",
      "Episode: 4100 35.0  Epsilon: 0.96  Mean Rewards 23.4\n",
      "Episode: 4200 19.0  Epsilon: 0.96  Mean Rewards 23.1\n",
      "Episode: 4300 13.0  Epsilon: 0.96  Mean Rewards 25.7\n",
      "Episode: 4400 20.0  Epsilon: 0.96  Mean Rewards 24.9\n",
      "Episode: 4500 27.0  Epsilon: 0.96  Mean Rewards 23.6\n",
      "Episode: 4600 15.0  Epsilon: 0.95  Mean Rewards 24.3\n",
      "Episode: 4700 76.0  Epsilon: 0.95  Mean Rewards 23.6\n",
      "Episode: 4800 27.0  Epsilon: 0.95  Mean Rewards 25.2\n",
      "Episode: 4900 22.0  Epsilon: 0.95  Mean Rewards 23.8\n",
      "Episode: 5000 24.0  Epsilon: 0.95  Mean Rewards 24.0\n",
      "Episode: 5100 21.0  Epsilon: 0.95  Mean Rewards 23.8\n",
      "Episode: 5200 17.0  Epsilon: 0.95  Mean Rewards 22.9\n",
      "Episode: 5300 17.0  Epsilon: 0.95  Mean Rewards 26.5\n",
      "Episode: 5400 28.0  Epsilon: 0.95  Mean Rewards 24.9\n",
      "Episode: 5500 15.0  Epsilon: 0.95  Mean Rewards 24.1\n",
      "Episode: 5600 26.0  Epsilon: 0.94  Mean Rewards 21.7\n",
      "Episode: 5700 9.0  Epsilon: 0.94  Mean Rewards 23.9\n",
      "Episode: 5800 48.0  Epsilon: 0.94  Mean Rewards 25.0\n",
      "Episode: 5900 28.0  Epsilon: 0.94  Mean Rewards 24.3\n",
      "Episode: 6000 22.0  Epsilon: 0.94  Mean Rewards 24.9\n",
      "Episode: 6100 13.0  Epsilon: 0.94  Mean Rewards 23.3\n",
      "Episode: 6200 46.0  Epsilon: 0.94  Mean Rewards 24.1\n",
      "Episode: 6300 18.0  Epsilon: 0.94  Mean Rewards 25.4\n",
      "Episode: 6400 19.0  Epsilon: 0.94  Mean Rewards 24.4\n",
      "Episode: 6500 15.0  Epsilon: 0.94  Mean Rewards 24.7\n",
      "Episode: 6600 30.0  Epsilon: 0.93  Mean Rewards 24.1\n",
      "Episode: 6700 20.0  Epsilon: 0.93  Mean Rewards 25.4\n",
      "Episode: 6800 23.0  Epsilon: 0.93  Mean Rewards 24.6\n",
      "Episode: 6900 20.0  Epsilon: 0.93  Mean Rewards 25.6\n",
      "Episode: 7000 18.0  Epsilon: 0.93  Mean Rewards 22.5\n",
      "Episode: 7100 14.0  Epsilon: 0.93  Mean Rewards 24.0\n",
      "Episode: 7200 14.0  Epsilon: 0.93  Mean Rewards 25.0\n",
      "Episode: 7300 35.0  Epsilon: 0.93  Mean Rewards 25.0\n",
      "Episode: 7400 19.0  Epsilon: 0.93  Mean Rewards 24.6\n",
      "Episode: 7500 51.0  Epsilon: 0.93  Mean Rewards 25.6\n",
      "Episode: 7600 10.0  Epsilon: 0.92  Mean Rewards 23.5\n",
      "Episode: 7700 26.0  Epsilon: 0.92  Mean Rewards 26.6\n",
      "Episode: 7800 10.0  Epsilon: 0.92  Mean Rewards 27.2\n",
      "Episode: 7900 24.0  Epsilon: 0.92  Mean Rewards 24.8\n",
      "Episode: 8000 16.0  Epsilon: 0.92  Mean Rewards 25.3\n",
      "Episode: 8100 20.0  Epsilon: 0.92  Mean Rewards 25.7\n",
      "Episode: 8200 17.0  Epsilon: 0.92  Mean Rewards 28.4\n",
      "Episode: 8300 79.0  Epsilon: 0.92  Mean Rewards 24.1\n",
      "Episode: 8400 23.0  Epsilon: 0.92  Mean Rewards 26.3\n",
      "Episode: 8500 82.0  Epsilon: 0.92  Mean Rewards 26.3\n",
      "Episode: 8600 22.0  Epsilon: 0.91  Mean Rewards 24.1\n",
      "Episode: 8700 22.0  Epsilon: 0.91  Mean Rewards 27.2\n",
      "Episode: 8800 31.0  Epsilon: 0.91  Mean Rewards 27.4\n",
      "Episode: 8900 14.0  Epsilon: 0.91  Mean Rewards 25.6\n",
      "Episode: 9000 10.0  Epsilon: 0.91  Mean Rewards 26.5\n",
      "Episode: 9100 19.0  Epsilon: 0.91  Mean Rewards 27.5\n",
      "Episode: 9200 20.0  Epsilon: 0.91  Mean Rewards 25.4\n",
      "Episode: 9300 30.0  Epsilon: 0.91  Mean Rewards 29.6\n",
      "Episode: 9400 9.0  Epsilon: 0.91  Mean Rewards 24.9\n",
      "Episode: 9500 21.0  Epsilon: 0.91  Mean Rewards 27.6\n",
      "Episode: 9600 14.0  Epsilon: 0.90  Mean Rewards 24.5\n",
      "Episode: 9700 31.0  Epsilon: 0.90  Mean Rewards 24.6\n",
      "Episode: 9800 36.0  Epsilon: 0.90  Mean Rewards 26.4\n",
      "Episode: 9900 22.0  Epsilon: 0.90  Mean Rewards 25.3\n",
      "Episode: 10000 20.0  Epsilon: 0.90  Mean Rewards 25.8\n",
      "Episode: 10100 16.0  Epsilon: 0.90  Mean Rewards 26.6\n",
      "Episode: 10200 13.0  Epsilon: 0.90  Mean Rewards 26.7\n",
      "Episode: 10300 28.0  Epsilon: 0.90  Mean Rewards 27.9\n",
      "Episode: 10400 33.0  Epsilon: 0.90  Mean Rewards 26.5\n",
      "Episode: 10500 21.0  Epsilon: 0.90  Mean Rewards 26.7\n",
      "Episode: 10600 62.0  Epsilon: 0.89  Mean Rewards 31.1\n",
      "Episode: 10700 56.0  Epsilon: 0.89  Mean Rewards 30.9\n",
      "Episode: 10800 24.0  Epsilon: 0.89  Mean Rewards 26.5\n",
      "Episode: 10900 21.0  Epsilon: 0.89  Mean Rewards 25.9\n",
      "Episode: 11000 29.0  Epsilon: 0.89  Mean Rewards 25.9\n",
      "Episode: 11100 37.0  Epsilon: 0.89  Mean Rewards 27.4\n",
      "Episode: 11200 28.0  Epsilon: 0.89  Mean Rewards 25.6\n",
      "Episode: 11300 49.0  Epsilon: 0.89  Mean Rewards 26.2\n",
      "Episode: 11400 21.0  Epsilon: 0.89  Mean Rewards 29.5\n",
      "Episode: 11500 47.0  Epsilon: 0.89  Mean Rewards 25.5\n",
      "Episode: 11600 31.0  Epsilon: 0.88  Mean Rewards 30.4\n",
      "Episode: 11700 12.0  Epsilon: 0.88  Mean Rewards 26.9\n",
      "Episode: 11800 14.0  Epsilon: 0.88  Mean Rewards 28.1\n",
      "Episode: 11900 18.0  Epsilon: 0.88  Mean Rewards 29.0\n",
      "Episode: 12000 12.0  Epsilon: 0.88  Mean Rewards 29.6\n",
      "Episode: 12100 28.0  Epsilon: 0.88  Mean Rewards 29.1\n",
      "Episode: 12200 16.0  Epsilon: 0.88  Mean Rewards 29.3\n",
      "Episode: 12300 25.0  Epsilon: 0.88  Mean Rewards 29.0\n",
      "Episode: 12400 28.0  Epsilon: 0.88  Mean Rewards 26.6\n",
      "Episode: 12500 34.0  Epsilon: 0.88  Mean Rewards 27.1\n",
      "Episode: 12600 12.0  Epsilon: 0.87  Mean Rewards 28.3\n",
      "Episode: 12700 28.0  Epsilon: 0.87  Mean Rewards 25.3\n",
      "Episode: 12800 42.0  Epsilon: 0.87  Mean Rewards 27.9\n",
      "Episode: 12900 35.0  Epsilon: 0.87  Mean Rewards 28.1\n",
      "Episode: 13000 34.0  Epsilon: 0.87  Mean Rewards 27.7\n",
      "Episode: 13100 26.0  Epsilon: 0.87  Mean Rewards 31.5\n",
      "Episode: 13200 76.0  Epsilon: 0.87  Mean Rewards 31.6\n",
      "Episode: 13300 9.0  Epsilon: 0.87  Mean Rewards 30.7\n",
      "Episode: 13400 29.0  Epsilon: 0.87  Mean Rewards 29.4\n",
      "Episode: 13500 29.0  Epsilon: 0.87  Mean Rewards 26.6\n",
      "Episode: 13600 13.0  Epsilon: 0.86  Mean Rewards 31.7\n",
      "Episode: 13700 42.0  Epsilon: 0.86  Mean Rewards 27.5\n",
      "Episode: 13800 21.0  Epsilon: 0.86  Mean Rewards 29.5\n",
      "Episode: 13900 24.0  Epsilon: 0.86  Mean Rewards 32.1\n",
      "Episode: 14000 10.0  Epsilon: 0.86  Mean Rewards 30.0\n",
      "Episode: 14100 20.0  Epsilon: 0.86  Mean Rewards 32.7\n",
      "Episode: 14200 17.0  Epsilon: 0.86  Mean Rewards 29.7\n",
      "Episode: 14300 17.0  Epsilon: 0.86  Mean Rewards 30.0\n",
      "Episode: 14400 13.0  Epsilon: 0.86  Mean Rewards 26.8\n",
      "Episode: 14500 25.0  Epsilon: 0.86  Mean Rewards 28.9\n",
      "Episode: 14600 31.0  Epsilon: 0.85  Mean Rewards 26.6\n",
      "Episode: 14700 38.0  Epsilon: 0.85  Mean Rewards 31.7\n",
      "Episode: 14800 46.0  Epsilon: 0.85  Mean Rewards 30.6\n",
      "Episode: 14900 34.0  Epsilon: 0.85  Mean Rewards 30.3\n",
      "Episode: 15000 25.0  Epsilon: 0.85  Mean Rewards 26.2\n",
      "Episode: 15100 15.0  Epsilon: 0.85  Mean Rewards 31.6\n",
      "Episode: 15200 21.0  Epsilon: 0.85  Mean Rewards 30.6\n",
      "Episode: 15300 69.0  Epsilon: 0.85  Mean Rewards 27.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 15400 22.0  Epsilon: 0.85  Mean Rewards 29.4\n",
      "Episode: 15500 11.0  Epsilon: 0.85  Mean Rewards 30.8\n",
      "Episode: 15600 58.0  Epsilon: 0.84  Mean Rewards 29.0\n",
      "Episode: 15700 28.0  Epsilon: 0.84  Mean Rewards 28.5\n",
      "Episode: 15800 19.0  Epsilon: 0.84  Mean Rewards 29.9\n",
      "Episode: 15900 35.0  Epsilon: 0.84  Mean Rewards 32.6\n",
      "Episode: 16000 28.0  Epsilon: 0.84  Mean Rewards 33.3\n",
      "Episode: 16100 30.0  Epsilon: 0.84  Mean Rewards 30.9\n",
      "Episode: 16200 53.0  Epsilon: 0.84  Mean Rewards 33.5\n",
      "Episode: 16300 13.0  Epsilon: 0.84  Mean Rewards 29.9\n",
      "Episode: 16400 23.0  Epsilon: 0.84  Mean Rewards 34.1\n",
      "Episode: 16500 55.0  Epsilon: 0.84  Mean Rewards 31.8\n",
      "Episode: 16600 14.0  Epsilon: 0.83  Mean Rewards 30.9\n",
      "Episode: 16700 13.0  Epsilon: 0.83  Mean Rewards 33.1\n",
      "Episode: 16800 45.0  Epsilon: 0.83  Mean Rewards 31.5\n",
      "Episode: 16900 15.0  Epsilon: 0.83  Mean Rewards 32.7\n",
      "Episode: 17000 36.0  Epsilon: 0.83  Mean Rewards 29.0\n",
      "Episode: 17100 14.0  Epsilon: 0.83  Mean Rewards 30.1\n",
      "Episode: 17200 36.0  Epsilon: 0.83  Mean Rewards 34.4\n",
      "Episode: 17300 19.0  Epsilon: 0.83  Mean Rewards 29.1\n",
      "Episode: 17400 15.0  Epsilon: 0.83  Mean Rewards 31.8\n",
      "Episode: 17500 49.0  Epsilon: 0.83  Mean Rewards 30.1\n",
      "Episode: 17600 41.0  Epsilon: 0.82  Mean Rewards 34.6\n",
      "Episode: 17700 14.0  Epsilon: 0.82  Mean Rewards 32.3\n",
      "Episode: 17800 89.0  Epsilon: 0.82  Mean Rewards 29.7\n",
      "Episode: 17900 38.0  Epsilon: 0.82  Mean Rewards 35.6\n",
      "Episode: 18000 14.0  Epsilon: 0.82  Mean Rewards 34.5\n",
      "Episode: 18100 62.0  Epsilon: 0.82  Mean Rewards 33.1\n",
      "Episode: 18200 36.0  Epsilon: 0.82  Mean Rewards 32.3\n",
      "Episode: 18300 10.0  Epsilon: 0.82  Mean Rewards 31.3\n",
      "Episode: 18400 19.0  Epsilon: 0.82  Mean Rewards 33.5\n",
      "Episode: 18500 25.0  Epsilon: 0.82  Mean Rewards 36.1\n",
      "Episode: 18600 21.0  Epsilon: 0.81  Mean Rewards 34.6\n",
      "Episode: 18700 29.0  Epsilon: 0.81  Mean Rewards 36.1\n",
      "Episode: 18800 60.0  Epsilon: 0.81  Mean Rewards 33.3\n",
      "Episode: 18900 13.0  Epsilon: 0.81  Mean Rewards 26.8\n",
      "Episode: 19000 28.0  Epsilon: 0.81  Mean Rewards 31.8\n",
      "Episode: 19100 17.0  Epsilon: 0.81  Mean Rewards 32.0\n",
      "Episode: 19200 23.0  Epsilon: 0.81  Mean Rewards 30.7\n",
      "Episode: 19300 39.0  Epsilon: 0.81  Mean Rewards 30.6\n",
      "Episode: 19400 34.0  Epsilon: 0.81  Mean Rewards 36.5\n",
      "Episode: 19500 17.0  Epsilon: 0.81  Mean Rewards 32.8\n",
      "Episode: 19600 47.0  Epsilon: 0.80  Mean Rewards 33.4\n",
      "Episode: 19700 21.0  Epsilon: 0.80  Mean Rewards 32.0\n",
      "Episode: 19800 63.0  Epsilon: 0.80  Mean Rewards 32.9\n",
      "Episode: 19900 13.0  Epsilon: 0.80  Mean Rewards 30.5\n",
      "Episode: 20000 13.0  Epsilon: 0.80  Mean Rewards 33.7\n",
      "Episode: 20100 32.0  Epsilon: 0.80  Mean Rewards 34.6\n",
      "Episode: 20200 29.0  Epsilon: 0.80  Mean Rewards 33.7\n",
      "Episode: 20300 22.0  Epsilon: 0.80  Mean Rewards 37.1\n",
      "Episode: 20400 13.0  Epsilon: 0.80  Mean Rewards 34.4\n",
      "Episode: 20500 17.0  Epsilon: 0.80  Mean Rewards 30.6\n",
      "Episode: 20600 15.0  Epsilon: 0.79  Mean Rewards 32.5\n",
      "Episode: 20700 45.0  Epsilon: 0.79  Mean Rewards 31.3\n",
      "Episode: 20800 69.0  Epsilon: 0.79  Mean Rewards 32.6\n",
      "Episode: 20900 122.0  Epsilon: 0.79  Mean Rewards 37.3\n",
      "Episode: 21000 29.0  Epsilon: 0.79  Mean Rewards 34.3\n",
      "Episode: 21100 20.0  Epsilon: 0.79  Mean Rewards 37.2\n",
      "Episode: 21200 35.0  Epsilon: 0.79  Mean Rewards 34.0\n",
      "Episode: 21300 15.0  Epsilon: 0.79  Mean Rewards 37.0\n",
      "Episode: 21400 26.0  Epsilon: 0.79  Mean Rewards 31.8\n",
      "Episode: 21500 34.0  Epsilon: 0.79  Mean Rewards 34.7\n",
      "Episode: 21600 77.0  Epsilon: 0.78  Mean Rewards 37.4\n",
      "Episode: 21700 12.0  Epsilon: 0.78  Mean Rewards 35.4\n",
      "Episode: 21800 60.0  Epsilon: 0.78  Mean Rewards 34.8\n",
      "Episode: 21900 16.0  Epsilon: 0.78  Mean Rewards 37.3\n",
      "Episode: 22000 78.0  Epsilon: 0.78  Mean Rewards 34.8\n",
      "Episode: 22100 65.0  Epsilon: 0.78  Mean Rewards 35.0\n",
      "Episode: 22200 74.0  Epsilon: 0.78  Mean Rewards 38.6\n",
      "Episode: 22300 13.0  Epsilon: 0.78  Mean Rewards 35.9\n",
      "Episode: 22400 24.0  Epsilon: 0.78  Mean Rewards 31.8\n",
      "Episode: 22500 35.0  Epsilon: 0.78  Mean Rewards 36.3\n",
      "Episode: 22600 51.0  Epsilon: 0.77  Mean Rewards 35.3\n",
      "Episode: 22700 57.0  Epsilon: 0.77  Mean Rewards 38.1\n",
      "Episode: 22800 14.0  Epsilon: 0.77  Mean Rewards 33.9\n",
      "Episode: 22900 14.0  Epsilon: 0.77  Mean Rewards 35.7\n",
      "Episode: 23000 21.0  Epsilon: 0.77  Mean Rewards 38.2\n",
      "Episode: 23100 76.0  Epsilon: 0.77  Mean Rewards 37.8\n",
      "Episode: 23200 67.0  Epsilon: 0.77  Mean Rewards 33.0\n",
      "Episode: 23300 14.0  Epsilon: 0.77  Mean Rewards 39.8\n",
      "Episode: 23400 13.0  Epsilon: 0.77  Mean Rewards 36.9\n",
      "Episode: 23500 42.0  Epsilon: 0.77  Mean Rewards 35.6\n",
      "Episode: 23600 24.0  Epsilon: 0.76  Mean Rewards 37.4\n",
      "Episode: 23700 36.0  Epsilon: 0.76  Mean Rewards 37.1\n",
      "Episode: 23800 32.0  Epsilon: 0.76  Mean Rewards 37.0\n",
      "Episode: 23900 13.0  Epsilon: 0.76  Mean Rewards 37.2\n",
      "Episode: 24000 46.0  Epsilon: 0.76  Mean Rewards 34.5\n",
      "Episode: 24100 9.0  Epsilon: 0.76  Mean Rewards 36.1\n",
      "Episode: 24200 111.0  Epsilon: 0.76  Mean Rewards 34.6\n",
      "Episode: 24300 71.0  Epsilon: 0.76  Mean Rewards 35.0\n",
      "Episode: 24400 34.0  Epsilon: 0.76  Mean Rewards 40.0\n",
      "Episode: 24500 18.0  Epsilon: 0.76  Mean Rewards 39.0\n",
      "Episode: 24600 25.0  Epsilon: 0.75  Mean Rewards 40.0\n",
      "Episode: 24700 23.0  Epsilon: 0.75  Mean Rewards 33.2\n",
      "Episode: 24800 14.0  Epsilon: 0.75  Mean Rewards 39.6\n",
      "Episode: 24900 30.0  Epsilon: 0.75  Mean Rewards 37.9\n",
      "Episode: 25000 110.0  Epsilon: 0.75  Mean Rewards 36.3\n",
      "Episode: 25100 11.0  Epsilon: 0.75  Mean Rewards 40.1\n",
      "Episode: 25200 38.0  Epsilon: 0.75  Mean Rewards 35.6\n",
      "Episode: 25300 29.0  Epsilon: 0.75  Mean Rewards 39.5\n",
      "Episode: 25400 55.0  Epsilon: 0.75  Mean Rewards 38.7\n",
      "Episode: 25500 41.0  Epsilon: 0.75  Mean Rewards 40.8\n",
      "Episode: 25600 39.0  Epsilon: 0.74  Mean Rewards 42.8\n",
      "Episode: 25700 22.0  Epsilon: 0.74  Mean Rewards 37.3\n",
      "Episode: 25800 67.0  Epsilon: 0.74  Mean Rewards 37.6\n",
      "Episode: 25900 20.0  Epsilon: 0.74  Mean Rewards 38.7\n",
      "Episode: 26000 25.0  Epsilon: 0.74  Mean Rewards 38.5\n",
      "Episode: 26100 53.0  Epsilon: 0.74  Mean Rewards 39.4\n",
      "Episode: 26200 49.0  Epsilon: 0.74  Mean Rewards 41.6\n",
      "Episode: 26300 39.0  Epsilon: 0.74  Mean Rewards 37.7\n",
      "Episode: 26400 12.0  Epsilon: 0.74  Mean Rewards 42.0\n",
      "Episode: 26500 49.0  Epsilon: 0.74  Mean Rewards 39.3\n",
      "Episode: 26600 17.0  Epsilon: 0.73  Mean Rewards 39.1\n",
      "Episode: 26700 34.0  Epsilon: 0.73  Mean Rewards 39.0\n",
      "Episode: 26800 22.0  Epsilon: 0.73  Mean Rewards 40.9\n",
      "Episode: 26900 51.0  Epsilon: 0.73  Mean Rewards 38.8\n",
      "Episode: 27000 48.0  Epsilon: 0.73  Mean Rewards 38.1\n",
      "Episode: 27100 14.0  Epsilon: 0.73  Mean Rewards 42.3\n",
      "Episode: 27200 18.0  Epsilon: 0.73  Mean Rewards 37.0\n",
      "Episode: 27300 25.0  Epsilon: 0.73  Mean Rewards 39.1\n",
      "Episode: 27400 14.0  Epsilon: 0.73  Mean Rewards 46.4\n",
      "Episode: 27500 41.0  Epsilon: 0.73  Mean Rewards 39.5\n",
      "Episode: 27600 28.0  Epsilon: 0.72  Mean Rewards 40.6\n",
      "Episode: 27700 36.0  Epsilon: 0.72  Mean Rewards 42.6\n",
      "Episode: 27800 90.0  Epsilon: 0.72  Mean Rewards 38.8\n",
      "Episode: 27900 13.0  Epsilon: 0.72  Mean Rewards 41.4\n",
      "Episode: 28000 11.0  Epsilon: 0.72  Mean Rewards 44.5\n",
      "Episode: 28100 35.0  Epsilon: 0.72  Mean Rewards 43.9\n",
      "Episode: 28200 29.0  Epsilon: 0.72  Mean Rewards 41.7\n",
      "Episode: 28300 34.0  Epsilon: 0.72  Mean Rewards 42.4\n",
      "Episode: 28400 122.0  Epsilon: 0.72  Mean Rewards 43.8\n",
      "Episode: 28500 137.0  Epsilon: 0.72  Mean Rewards 47.8\n",
      "Episode: 28600 12.0  Epsilon: 0.71  Mean Rewards 40.9\n",
      "Episode: 28700 12.0  Epsilon: 0.71  Mean Rewards 45.5\n",
      "Episode: 28800 73.0  Epsilon: 0.71  Mean Rewards 44.5\n",
      "Episode: 28900 14.0  Epsilon: 0.71  Mean Rewards 46.9\n",
      "Episode: 29000 42.0  Epsilon: 0.71  Mean Rewards 43.4\n",
      "Episode: 29100 14.0  Epsilon: 0.71  Mean Rewards 41.4\n",
      "Episode: 29200 50.0  Epsilon: 0.71  Mean Rewards 39.5\n",
      "Episode: 29300 37.0  Epsilon: 0.71  Mean Rewards 45.7\n",
      "Episode: 29400 61.0  Epsilon: 0.71  Mean Rewards 43.9\n",
      "Episode: 29500 64.0  Epsilon: 0.71  Mean Rewards 43.5\n",
      "Episode: 29600 19.0  Epsilon: 0.70  Mean Rewards 42.4\n",
      "Episode: 29700 41.0  Epsilon: 0.70  Mean Rewards 43.1\n",
      "Episode: 29800 23.0  Epsilon: 0.70  Mean Rewards 48.3\n",
      "Episode: 29900 124.0  Epsilon: 0.70  Mean Rewards 47.0\n",
      "Episode: 30000 54.0  Epsilon: 0.70  Mean Rewards 40.2\n",
      "Episode: 30100 30.0  Epsilon: 0.70  Mean Rewards 46.0\n",
      "Episode: 30200 31.0  Epsilon: 0.70  Mean Rewards 41.1\n",
      "Episode: 30300 40.0  Epsilon: 0.70  Mean Rewards 47.0\n",
      "Episode: 30400 70.0  Epsilon: 0.70  Mean Rewards 42.2\n",
      "Episode: 30500 27.0  Epsilon: 0.70  Mean Rewards 47.1\n",
      "Episode: 30600 94.0  Epsilon: 0.69  Mean Rewards 42.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 30700 45.0  Epsilon: 0.69  Mean Rewards 45.0\n",
      "Episode: 30800 18.0  Epsilon: 0.69  Mean Rewards 44.5\n",
      "Episode: 30900 32.0  Epsilon: 0.69  Mean Rewards 45.0\n",
      "Episode: 31000 35.0  Epsilon: 0.69  Mean Rewards 45.1\n",
      "Episode: 31100 13.0  Epsilon: 0.69  Mean Rewards 41.1\n",
      "Episode: 31200 19.0  Epsilon: 0.69  Mean Rewards 49.6\n",
      "Episode: 31300 33.0  Epsilon: 0.69  Mean Rewards 44.9\n",
      "Episode: 31400 60.0  Epsilon: 0.69  Mean Rewards 47.9\n",
      "Episode: 31500 36.0  Epsilon: 0.69  Mean Rewards 49.5\n",
      "Episode: 31600 84.0  Epsilon: 0.68  Mean Rewards 45.1\n",
      "Episode: 31700 76.0  Epsilon: 0.68  Mean Rewards 49.5\n",
      "Episode: 31800 45.0  Epsilon: 0.68  Mean Rewards 46.7\n",
      "Episode: 31900 30.0  Epsilon: 0.68  Mean Rewards 52.6\n",
      "Episode: 32000 96.0  Epsilon: 0.68  Mean Rewards 45.1\n",
      "Episode: 32100 38.0  Epsilon: 0.68  Mean Rewards 46.7\n",
      "Episode: 32200 12.0  Epsilon: 0.68  Mean Rewards 43.3\n",
      "Episode: 32300 31.0  Epsilon: 0.68  Mean Rewards 46.4\n",
      "Episode: 32400 23.0  Epsilon: 0.68  Mean Rewards 48.7\n",
      "Episode: 32500 15.0  Epsilon: 0.68  Mean Rewards 50.9\n",
      "Episode: 32600 16.0  Epsilon: 0.67  Mean Rewards 45.8\n",
      "Episode: 32700 84.0  Epsilon: 0.67  Mean Rewards 51.9\n",
      "Episode: 32800 29.0  Epsilon: 0.67  Mean Rewards 54.8\n",
      "Episode: 32900 22.0  Epsilon: 0.67  Mean Rewards 51.9\n",
      "Episode: 33000 72.0  Epsilon: 0.67  Mean Rewards 43.2\n",
      "Episode: 33100 32.0  Epsilon: 0.67  Mean Rewards 48.2\n",
      "Episode: 33200 20.0  Epsilon: 0.67  Mean Rewards 51.2\n",
      "Episode: 33300 25.0  Epsilon: 0.67  Mean Rewards 49.2\n",
      "Episode: 33400 51.0  Epsilon: 0.67  Mean Rewards 50.3\n",
      "Episode: 33500 49.0  Epsilon: 0.67  Mean Rewards 48.7\n",
      "Episode: 33600 26.0  Epsilon: 0.66  Mean Rewards 54.7\n",
      "Episode: 33700 27.0  Epsilon: 0.66  Mean Rewards 47.7\n",
      "Episode: 33800 26.0  Epsilon: 0.66  Mean Rewards 49.1\n",
      "Episode: 33900 21.0  Epsilon: 0.66  Mean Rewards 51.1\n",
      "Episode: 34000 13.0  Epsilon: 0.66  Mean Rewards 51.0\n",
      "Episode: 34100 97.0  Epsilon: 0.66  Mean Rewards 49.7\n",
      "Episode: 34200 40.0  Epsilon: 0.66  Mean Rewards 53.9\n",
      "Episode: 34300 10.0  Epsilon: 0.66  Mean Rewards 47.6\n",
      "Episode: 34400 14.0  Epsilon: 0.66  Mean Rewards 49.8\n",
      "Episode: 34500 68.0  Epsilon: 0.66  Mean Rewards 49.6\n",
      "Episode: 34600 87.0  Epsilon: 0.65  Mean Rewards 48.4\n",
      "Episode: 34700 31.0  Epsilon: 0.65  Mean Rewards 55.0\n",
      "Episode: 34800 14.0  Epsilon: 0.65  Mean Rewards 46.9\n",
      "Episode: 34900 20.0  Epsilon: 0.65  Mean Rewards 46.5\n",
      "Episode: 35000 12.0  Epsilon: 0.65  Mean Rewards 47.3\n",
      "Episode: 35100 11.0  Epsilon: 0.65  Mean Rewards 52.5\n",
      "Episode: 35200 29.0  Epsilon: 0.65  Mean Rewards 46.2\n",
      "Episode: 35300 80.0  Epsilon: 0.65  Mean Rewards 50.0\n",
      "Episode: 35400 37.0  Epsilon: 0.65  Mean Rewards 49.8\n",
      "Episode: 35500 41.0  Epsilon: 0.65  Mean Rewards 54.6\n",
      "Episode: 35600 98.0  Epsilon: 0.64  Mean Rewards 51.5\n",
      "Episode: 35700 56.0  Epsilon: 0.64  Mean Rewards 49.7\n",
      "Episode: 35800 47.0  Epsilon: 0.64  Mean Rewards 61.0\n",
      "Episode: 35900 16.0  Epsilon: 0.64  Mean Rewards 60.7\n",
      "Episode: 36000 42.0  Epsilon: 0.64  Mean Rewards 51.3\n",
      "Episode: 36100 26.0  Epsilon: 0.64  Mean Rewards 49.3\n",
      "Episode: 36200 39.0  Epsilon: 0.64  Mean Rewards 54.7\n",
      "Episode: 36300 21.0  Epsilon: 0.64  Mean Rewards 52.7\n",
      "Episode: 36400 11.0  Epsilon: 0.64  Mean Rewards 56.2\n",
      "Episode: 36500 51.0  Epsilon: 0.64  Mean Rewards 55.6\n",
      "Episode: 36600 88.0  Epsilon: 0.63  Mean Rewards 53.5\n",
      "Episode: 36700 149.0  Epsilon: 0.63  Mean Rewards 56.1\n",
      "Episode: 36800 132.0  Epsilon: 0.63  Mean Rewards 54.8\n",
      "Episode: 36900 120.0  Epsilon: 0.63  Mean Rewards 50.1\n",
      "Episode: 37000 129.0  Epsilon: 0.63  Mean Rewards 59.4\n",
      "Episode: 37100 28.0  Epsilon: 0.63  Mean Rewards 48.0\n",
      "Episode: 37200 65.0  Epsilon: 0.63  Mean Rewards 52.7\n",
      "Episode: 37300 63.0  Epsilon: 0.63  Mean Rewards 44.0\n",
      "Episode: 37400 62.0  Epsilon: 0.63  Mean Rewards 57.8\n",
      "Episode: 37500 33.0  Epsilon: 0.63  Mean Rewards 59.6\n",
      "Episode: 37600 18.0  Epsilon: 0.62  Mean Rewards 50.7\n",
      "Episode: 37700 24.0  Epsilon: 0.62  Mean Rewards 47.1\n",
      "Episode: 37800 21.0  Epsilon: 0.62  Mean Rewards 52.2\n",
      "Episode: 37900 124.0  Epsilon: 0.62  Mean Rewards 51.7\n",
      "Episode: 38000 38.0  Epsilon: 0.62  Mean Rewards 60.1\n",
      "Episode: 38100 58.0  Epsilon: 0.62  Mean Rewards 58.3\n",
      "Episode: 38200 16.0  Epsilon: 0.62  Mean Rewards 58.1\n",
      "Episode: 38300 24.0  Epsilon: 0.62  Mean Rewards 63.4\n",
      "Episode: 38400 23.0  Epsilon: 0.62  Mean Rewards 47.2\n",
      "Episode: 38500 46.0  Epsilon: 0.62  Mean Rewards 47.5\n",
      "Episode: 38600 133.0  Epsilon: 0.61  Mean Rewards 59.9\n",
      "Episode: 38700 114.0  Epsilon: 0.61  Mean Rewards 52.5\n",
      "Episode: 38800 74.0  Epsilon: 0.61  Mean Rewards 60.8\n",
      "Episode: 38900 34.0  Epsilon: 0.61  Mean Rewards 53.1\n",
      "Episode: 39000 38.0  Epsilon: 0.61  Mean Rewards 57.8\n",
      "Episode: 39100 34.0  Epsilon: 0.61  Mean Rewards 61.4\n",
      "Episode: 39200 18.0  Epsilon: 0.61  Mean Rewards 62.6\n",
      "Episode: 39300 60.0  Epsilon: 0.61  Mean Rewards 56.3\n",
      "Episode: 39400 43.0  Epsilon: 0.61  Mean Rewards 57.6\n",
      "Episode: 39500 16.0  Epsilon: 0.61  Mean Rewards 52.8\n",
      "Episode: 39600 175.0  Epsilon: 0.60  Mean Rewards 61.1\n",
      "Episode: 39700 33.0  Epsilon: 0.60  Mean Rewards 62.0\n",
      "Episode: 39800 14.0  Epsilon: 0.60  Mean Rewards 57.2\n",
      "Episode: 39900 100.0  Epsilon: 0.60  Mean Rewards 59.7\n",
      "Episode: 40000 44.0  Epsilon: 0.60  Mean Rewards 57.2\n",
      "Episode: 40100 38.0  Epsilon: 0.60  Mean Rewards 53.6\n",
      "Episode: 40200 33.0  Epsilon: 0.60  Mean Rewards 46.6\n",
      "Episode: 40300 10.0  Epsilon: 0.60  Mean Rewards 64.7\n",
      "Episode: 40400 152.0  Epsilon: 0.60  Mean Rewards 62.5\n",
      "Episode: 40500 31.0  Epsilon: 0.60  Mean Rewards 56.1\n",
      "Episode: 40600 23.0  Epsilon: 0.59  Mean Rewards 58.4\n",
      "Episode: 40700 38.0  Epsilon: 0.59  Mean Rewards 64.5\n",
      "Episode: 40800 14.0  Epsilon: 0.59  Mean Rewards 57.8\n",
      "Episode: 40900 179.0  Epsilon: 0.59  Mean Rewards 66.1\n",
      "Episode: 41000 103.0  Epsilon: 0.59  Mean Rewards 65.2\n",
      "Episode: 41100 120.0  Epsilon: 0.59  Mean Rewards 69.8\n",
      "Episode: 41200 69.0  Epsilon: 0.59  Mean Rewards 58.6\n",
      "Episode: 41300 56.0  Epsilon: 0.59  Mean Rewards 63.1\n",
      "Episode: 41400 30.0  Epsilon: 0.59  Mean Rewards 63.9\n",
      "Episode: 41500 130.0  Epsilon: 0.59  Mean Rewards 67.5\n",
      "Episode: 41600 94.0  Epsilon: 0.58  Mean Rewards 65.1\n",
      "Episode: 41700 15.0  Epsilon: 0.58  Mean Rewards 64.2\n",
      "Episode: 41800 192.0  Epsilon: 0.58  Mean Rewards 69.2\n",
      "Episode: 41900 32.0  Epsilon: 0.58  Mean Rewards 63.6\n",
      "Episode: 42000 32.0  Epsilon: 0.58  Mean Rewards 68.7\n",
      "Episode: 42100 39.0  Epsilon: 0.58  Mean Rewards 55.8\n",
      "Episode: 42200 60.0  Epsilon: 0.58  Mean Rewards 73.1\n",
      "Episode: 42300 31.0  Epsilon: 0.58  Mean Rewards 66.9\n",
      "Episode: 42400 28.0  Epsilon: 0.58  Mean Rewards 63.2\n",
      "Episode: 42500 15.0  Epsilon: 0.58  Mean Rewards 56.9\n",
      "Episode: 42600 94.0  Epsilon: 0.57  Mean Rewards 61.9\n",
      "Episode: 42700 49.0  Epsilon: 0.57  Mean Rewards 65.8\n",
      "Episode: 42800 132.0  Epsilon: 0.57  Mean Rewards 71.8\n",
      "Episode: 42900 19.0  Epsilon: 0.57  Mean Rewards 70.8\n",
      "Episode: 43000 16.0  Epsilon: 0.57  Mean Rewards 76.6\n",
      "Episode: 43100 40.0  Epsilon: 0.57  Mean Rewards 61.6\n",
      "Episode: 43200 34.0  Epsilon: 0.57  Mean Rewards 61.3\n",
      "Episode: 43300 26.0  Epsilon: 0.57  Mean Rewards 72.2\n",
      "Episode: 43400 93.0  Epsilon: 0.57  Mean Rewards 73.8\n",
      "Episode: 43500 75.0  Epsilon: 0.57  Mean Rewards 77.1\n",
      "Episode: 43600 26.0  Epsilon: 0.56  Mean Rewards 72.2\n",
      "Episode: 43700 84.0  Epsilon: 0.56  Mean Rewards 66.6\n",
      "Episode: 43800 59.0  Epsilon: 0.56  Mean Rewards 79.5\n",
      "Episode: 43900 19.0  Epsilon: 0.56  Mean Rewards 63.0\n",
      "Episode: 44000 75.0  Epsilon: 0.56  Mean Rewards 71.2\n",
      "Episode: 44100 102.0  Epsilon: 0.56  Mean Rewards 64.0\n",
      "Episode: 44200 68.0  Epsilon: 0.56  Mean Rewards 72.3\n",
      "Episode: 44300 20.0  Epsilon: 0.56  Mean Rewards 76.6\n",
      "Episode: 44400 12.0  Epsilon: 0.56  Mean Rewards 71.9\n",
      "Episode: 44500 32.0  Epsilon: 0.56  Mean Rewards 75.7\n",
      "Episode: 44600 52.0  Epsilon: 0.55  Mean Rewards 71.1\n",
      "Episode: 44700 78.0  Epsilon: 0.55  Mean Rewards 80.9\n",
      "Episode: 44800 33.0  Epsilon: 0.55  Mean Rewards 79.0\n",
      "Episode: 44900 93.0  Epsilon: 0.55  Mean Rewards 73.9\n",
      "Episode: 45000 61.0  Epsilon: 0.55  Mean Rewards 71.4\n",
      "Episode: 45100 32.0  Epsilon: 0.55  Mean Rewards 77.9\n",
      "Episode: 45200 25.0  Epsilon: 0.55  Mean Rewards 68.0\n",
      "Episode: 45300 202.0  Epsilon: 0.55  Mean Rewards 77.0\n",
      "Episode: 45400 49.0  Epsilon: 0.55  Mean Rewards 70.1\n",
      "Episode: 45500 17.0  Epsilon: 0.55  Mean Rewards 74.8\n",
      "Episode: 45600 111.0  Epsilon: 0.54  Mean Rewards 77.0\n",
      "Episode: 45700 56.0  Epsilon: 0.54  Mean Rewards 81.6\n",
      "Episode: 45800 22.0  Epsilon: 0.54  Mean Rewards 71.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 45900 74.0  Epsilon: 0.54  Mean Rewards 84.1\n",
      "Episode: 46000 179.0  Epsilon: 0.54  Mean Rewards 80.0\n",
      "Episode: 46100 177.0  Epsilon: 0.54  Mean Rewards 84.5\n",
      "Episode: 46200 10.0  Epsilon: 0.54  Mean Rewards 74.6\n",
      "Episode: 46300 59.0  Epsilon: 0.54  Mean Rewards 77.7\n",
      "Episode: 46400 102.0  Epsilon: 0.54  Mean Rewards 81.3\n",
      "Episode: 46500 11.0  Epsilon: 0.54  Mean Rewards 67.0\n",
      "Episode: 46600 98.0  Epsilon: 0.53  Mean Rewards 80.9\n",
      "Episode: 46700 153.0  Epsilon: 0.53  Mean Rewards 76.0\n",
      "Episode: 46800 147.0  Epsilon: 0.53  Mean Rewards 82.9\n",
      "Episode: 46900 129.0  Epsilon: 0.53  Mean Rewards 88.7\n",
      "Episode: 47000 88.0  Epsilon: 0.53  Mean Rewards 81.6\n",
      "Episode: 47100 50.0  Epsilon: 0.53  Mean Rewards 83.6\n",
      "Episode: 47200 128.0  Epsilon: 0.53  Mean Rewards 91.0\n",
      "Episode: 47300 65.0  Epsilon: 0.53  Mean Rewards 78.9\n",
      "Episode: 47400 135.0  Epsilon: 0.53  Mean Rewards 74.4\n",
      "Episode: 47500 115.0  Epsilon: 0.53  Mean Rewards 91.1\n",
      "Episode: 47600 113.0  Epsilon: 0.52  Mean Rewards 81.7\n",
      "Episode: 47700 65.0  Epsilon: 0.52  Mean Rewards 68.1\n",
      "Episode: 47800 103.0  Epsilon: 0.52  Mean Rewards 87.3\n",
      "Episode: 47900 145.0  Epsilon: 0.52  Mean Rewards 95.2\n",
      "Episode: 48000 38.0  Epsilon: 0.52  Mean Rewards 85.3\n",
      "Episode: 48100 123.0  Epsilon: 0.52  Mean Rewards 81.6\n",
      "Episode: 48200 114.0  Epsilon: 0.52  Mean Rewards 84.2\n",
      "Episode: 48300 29.0  Epsilon: 0.52  Mean Rewards 83.3\n",
      "Episode: 48400 73.0  Epsilon: 0.52  Mean Rewards 84.0\n",
      "Episode: 48500 41.0  Epsilon: 0.52  Mean Rewards 80.6\n",
      "Episode: 48600 59.0  Epsilon: 0.51  Mean Rewards 90.0\n",
      "Episode: 48700 61.0  Epsilon: 0.51  Mean Rewards 78.0\n",
      "Episode: 48800 163.0  Epsilon: 0.51  Mean Rewards 94.6\n",
      "Episode: 48900 124.0  Epsilon: 0.51  Mean Rewards 91.3\n",
      "Episode: 49000 67.0  Epsilon: 0.51  Mean Rewards 91.2\n",
      "Episode: 49100 26.0  Epsilon: 0.51  Mean Rewards 89.7\n",
      "Episode: 49200 15.0  Epsilon: 0.51  Mean Rewards 85.1\n",
      "Episode: 49300 26.0  Epsilon: 0.51  Mean Rewards 92.5\n",
      "Episode: 49400 45.0  Epsilon: 0.51  Mean Rewards 80.8\n",
      "Episode: 49500 115.0  Epsilon: 0.51  Mean Rewards 85.0\n",
      "Episode: 49600 169.0  Epsilon: 0.50  Mean Rewards 85.3\n",
      "Episode: 49700 173.0  Epsilon: 0.50  Mean Rewards 84.5\n",
      "Episode: 49800 128.0  Epsilon: 0.50  Mean Rewards 83.0\n",
      "Episode: 49900 289.0  Epsilon: 0.50  Mean Rewards 97.2\n",
      "Episode: 50000 189.0  Epsilon: 0.50  Mean Rewards 88.5\n",
      "Episode: 50100 73.0  Epsilon: 0.50  Mean Rewards 94.6\n",
      "Episode: 50200 69.0  Epsilon: 0.50  Mean Rewards 80.3\n",
      "Episode: 50300 17.0  Epsilon: 0.50  Mean Rewards 85.7\n",
      "Episode: 50400 103.0  Epsilon: 0.50  Mean Rewards 88.8\n",
      "Episode: 50500 43.0  Epsilon: 0.50  Mean Rewards 99.9\n",
      "Episode: 50600 83.0  Epsilon: 0.49  Mean Rewards 93.0\n",
      "Episode: 50700 49.0  Epsilon: 0.49  Mean Rewards 94.8\n",
      "Episode: 50800 68.0  Epsilon: 0.49  Mean Rewards 103.6\n",
      "Episode: 50900 125.0  Epsilon: 0.49  Mean Rewards 101.6\n",
      "Episode: 51000 69.0  Epsilon: 0.49  Mean Rewards 102.9\n",
      "Episode: 51100 238.0  Epsilon: 0.49  Mean Rewards 87.2\n",
      "Episode: 51200 23.0  Epsilon: 0.49  Mean Rewards 82.0\n",
      "Episode: 51300 47.0  Epsilon: 0.49  Mean Rewards 104.2\n",
      "Episode: 51400 132.0  Epsilon: 0.49  Mean Rewards 94.3\n",
      "Episode: 51500 65.0  Epsilon: 0.49  Mean Rewards 97.4\n",
      "Episode: 51600 95.0  Epsilon: 0.48  Mean Rewards 112.2\n",
      "Episode: 51700 81.0  Epsilon: 0.48  Mean Rewards 97.8\n",
      "Episode: 51800 103.0  Epsilon: 0.48  Mean Rewards 107.2\n",
      "Episode: 51900 73.0  Epsilon: 0.48  Mean Rewards 96.4\n",
      "Episode: 52000 71.0  Epsilon: 0.48  Mean Rewards 112.3\n",
      "Episode: 52100 215.0  Epsilon: 0.48  Mean Rewards 122.7\n",
      "Episode: 52200 60.0  Epsilon: 0.48  Mean Rewards 104.5\n",
      "Episode: 52300 88.0  Epsilon: 0.48  Mean Rewards 105.9\n",
      "Episode: 52400 68.0  Epsilon: 0.48  Mean Rewards 104.4\n",
      "Episode: 52500 246.0  Epsilon: 0.48  Mean Rewards 101.5\n",
      "Episode: 52600 97.0  Epsilon: 0.47  Mean Rewards 98.8\n",
      "Episode: 52700 35.0  Epsilon: 0.47  Mean Rewards 105.1\n",
      "Episode: 52800 84.0  Epsilon: 0.47  Mean Rewards 105.3\n",
      "Episode: 52900 132.0  Epsilon: 0.47  Mean Rewards 112.7\n",
      "Episode: 53000 102.0  Epsilon: 0.47  Mean Rewards 96.2\n",
      "Episode: 53100 88.0  Epsilon: 0.47  Mean Rewards 98.0\n",
      "Episode: 53200 102.0  Epsilon: 0.47  Mean Rewards 100.7\n",
      "Episode: 53300 231.0  Epsilon: 0.47  Mean Rewards 97.1\n",
      "Episode: 53400 136.0  Epsilon: 0.47  Mean Rewards 107.0\n",
      "Episode: 53500 75.0  Epsilon: 0.47  Mean Rewards 103.0\n",
      "Episode: 53600 103.0  Epsilon: 0.46  Mean Rewards 106.9\n",
      "Episode: 53700 22.0  Epsilon: 0.46  Mean Rewards 104.2\n",
      "Episode: 53800 20.0  Epsilon: 0.46  Mean Rewards 92.3\n",
      "Episode: 53900 214.0  Epsilon: 0.46  Mean Rewards 99.4\n",
      "Episode: 54000 17.0  Epsilon: 0.46  Mean Rewards 104.6\n",
      "Episode: 54100 49.0  Epsilon: 0.46  Mean Rewards 92.1\n",
      "Episode: 54200 190.0  Epsilon: 0.46  Mean Rewards 117.7\n",
      "Episode: 54300 133.0  Epsilon: 0.46  Mean Rewards 107.4\n",
      "Episode: 54400 141.0  Epsilon: 0.46  Mean Rewards 107.3\n",
      "Episode: 54500 227.0  Epsilon: 0.46  Mean Rewards 144.2\n",
      "Episode: 54600 37.0  Epsilon: 0.45  Mean Rewards 107.1\n",
      "Episode: 54700 27.0  Epsilon: 0.45  Mean Rewards 108.2\n",
      "Episode: 54800 175.0  Epsilon: 0.45  Mean Rewards 115.0\n",
      "Episode: 54900 70.0  Epsilon: 0.45  Mean Rewards 109.5\n",
      "Episode: 55000 256.0  Epsilon: 0.45  Mean Rewards 102.2\n",
      "Episode: 55100 99.0  Epsilon: 0.45  Mean Rewards 110.9\n",
      "Episode: 55200 47.0  Epsilon: 0.45  Mean Rewards 132.6\n",
      "Episode: 55300 91.0  Epsilon: 0.45  Mean Rewards 128.0\n",
      "Episode: 55400 147.0  Epsilon: 0.45  Mean Rewards 123.2\n",
      "Episode: 55500 54.0  Epsilon: 0.45  Mean Rewards 111.9\n",
      "Episode: 55600 84.0  Epsilon: 0.44  Mean Rewards 109.7\n",
      "Episode: 55700 103.0  Epsilon: 0.44  Mean Rewards 130.4\n",
      "Episode: 55800 90.0  Epsilon: 0.44  Mean Rewards 120.2\n",
      "Episode: 55900 162.0  Epsilon: 0.44  Mean Rewards 126.0\n",
      "Episode: 56000 41.0  Epsilon: 0.44  Mean Rewards 120.8\n",
      "Episode: 56100 161.0  Epsilon: 0.44  Mean Rewards 121.7\n",
      "Episode: 56200 149.0  Epsilon: 0.44  Mean Rewards 135.0\n",
      "Episode: 56300 132.0  Epsilon: 0.44  Mean Rewards 124.8\n",
      "Episode: 56400 310.0  Epsilon: 0.44  Mean Rewards 119.9\n",
      "Episode: 56500 91.0  Epsilon: 0.44  Mean Rewards 122.5\n",
      "Episode: 56600 75.0  Epsilon: 0.43  Mean Rewards 114.6\n",
      "Episode: 56700 96.0  Epsilon: 0.43  Mean Rewards 117.2\n",
      "Episode: 56800 136.0  Epsilon: 0.43  Mean Rewards 110.4\n",
      "Episode: 56900 143.0  Epsilon: 0.43  Mean Rewards 132.4\n",
      "Episode: 57000 16.0  Epsilon: 0.43  Mean Rewards 121.1\n",
      "Episode: 57100 250.0  Epsilon: 0.43  Mean Rewards 137.9\n",
      "Episode: 57200 177.0  Epsilon: 0.43  Mean Rewards 124.1\n",
      "Episode: 57300 133.0  Epsilon: 0.43  Mean Rewards 133.8\n",
      "Episode: 57400 169.0  Epsilon: 0.43  Mean Rewards 139.5\n",
      "Episode: 57500 59.0  Epsilon: 0.43  Mean Rewards 122.6\n",
      "Episode: 57600 35.0  Epsilon: 0.42  Mean Rewards 119.0\n",
      "Episode: 57700 71.0  Epsilon: 0.42  Mean Rewards 138.8\n",
      "Episode: 57800 219.0  Epsilon: 0.42  Mean Rewards 147.8\n",
      "Episode: 57900 10.0  Epsilon: 0.42  Mean Rewards 130.7\n",
      "Episode: 58000 188.0  Epsilon: 0.42  Mean Rewards 133.2\n",
      "Episode: 58100 165.0  Epsilon: 0.42  Mean Rewards 125.1\n",
      "Episode: 58200 180.0  Epsilon: 0.42  Mean Rewards 121.7\n",
      "Episode: 58300 210.0  Epsilon: 0.42  Mean Rewards 129.1\n",
      "Episode: 58400 196.0  Epsilon: 0.42  Mean Rewards 120.2\n",
      "Episode: 58500 88.0  Epsilon: 0.42  Mean Rewards 125.4\n",
      "Episode: 58600 190.0  Epsilon: 0.41  Mean Rewards 137.5\n",
      "Episode: 58700 236.0  Epsilon: 0.41  Mean Rewards 123.2\n",
      "Episode: 58800 67.0  Epsilon: 0.41  Mean Rewards 146.2\n",
      "Episode: 58900 120.0  Epsilon: 0.41  Mean Rewards 137.7\n",
      "Episode: 59000 30.0  Epsilon: 0.41  Mean Rewards 134.0\n",
      "Episode: 59100 469.0  Epsilon: 0.41  Mean Rewards 155.0\n",
      "Episode: 59200 171.0  Epsilon: 0.41  Mean Rewards 118.6\n",
      "Episode: 59300 121.0  Epsilon: 0.41  Mean Rewards 160.3\n",
      "Episode: 59400 253.0  Epsilon: 0.41  Mean Rewards 140.6\n",
      "Episode: 59500 25.0  Epsilon: 0.41  Mean Rewards 149.7\n",
      "Episode: 59600 161.0  Epsilon: 0.40  Mean Rewards 160.2\n",
      "Episode: 59700 27.0  Epsilon: 0.40  Mean Rewards 139.9\n",
      "Episode: 59800 141.0  Epsilon: 0.40  Mean Rewards 143.2\n",
      "Episode: 59900 55.0  Epsilon: 0.40  Mean Rewards 138.1\n",
      "Episode: 60000 256.0  Epsilon: 0.40  Mean Rewards 147.3\n",
      "Episode: 60100 86.0  Epsilon: 0.40  Mean Rewards 127.4\n",
      "Episode: 60200 283.0  Epsilon: 0.40  Mean Rewards 147.1\n",
      "Episode: 60300 151.0  Epsilon: 0.40  Mean Rewards 154.4\n",
      "Episode: 60400 20.0  Epsilon: 0.40  Mean Rewards 137.0\n",
      "Episode: 60500 63.0  Epsilon: 0.40  Mean Rewards 155.4\n",
      "Episode: 60600 87.0  Epsilon: 0.39  Mean Rewards 133.6\n",
      "Episode: 60700 172.0  Epsilon: 0.39  Mean Rewards 122.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 60800 75.0  Epsilon: 0.39  Mean Rewards 143.9\n",
      "Episode: 60900 265.0  Epsilon: 0.39  Mean Rewards 145.6\n",
      "Episode: 61000 326.0  Epsilon: 0.39  Mean Rewards 141.7\n",
      "Episode: 61100 253.0  Epsilon: 0.39  Mean Rewards 168.7\n",
      "Episode: 61200 90.0  Epsilon: 0.39  Mean Rewards 146.5\n",
      "Episode: 61300 98.0  Epsilon: 0.39  Mean Rewards 139.3\n",
      "Episode: 61400 13.0  Epsilon: 0.39  Mean Rewards 159.3\n",
      "Episode: 61500 130.0  Epsilon: 0.39  Mean Rewards 143.8\n",
      "Episode: 61600 18.0  Epsilon: 0.38  Mean Rewards 148.8\n",
      "Episode: 61700 244.0  Epsilon: 0.38  Mean Rewards 154.4\n",
      "Episode: 61800 164.0  Epsilon: 0.38  Mean Rewards 141.6\n",
      "Episode: 61900 357.0  Epsilon: 0.38  Mean Rewards 154.7\n",
      "Episode: 62000 144.0  Epsilon: 0.38  Mean Rewards 156.5\n",
      "Episode: 62100 195.0  Epsilon: 0.38  Mean Rewards 160.9\n",
      "Episode: 62200 105.0  Epsilon: 0.38  Mean Rewards 165.3\n",
      "Episode: 62300 305.0  Epsilon: 0.38  Mean Rewards 143.0\n",
      "Episode: 62400 128.0  Epsilon: 0.38  Mean Rewards 155.2\n",
      "Episode: 62500 43.0  Epsilon: 0.38  Mean Rewards 159.5\n",
      "Episode: 62600 21.0  Epsilon: 0.37  Mean Rewards 165.5\n",
      "Episode: 62700 13.0  Epsilon: 0.37  Mean Rewards 148.3\n",
      "Episode: 62800 136.0  Epsilon: 0.37  Mean Rewards 167.9\n",
      "Episode: 62900 108.0  Epsilon: 0.37  Mean Rewards 167.3\n",
      "Episode: 63000 84.0  Epsilon: 0.37  Mean Rewards 165.2\n",
      "Episode: 63100 186.0  Epsilon: 0.37  Mean Rewards 169.1\n",
      "Episode: 63200 82.0  Epsilon: 0.37  Mean Rewards 169.8\n",
      "Episode: 63300 255.0  Epsilon: 0.37  Mean Rewards 152.0\n",
      "Episode: 63400 353.0  Epsilon: 0.37  Mean Rewards 167.7\n",
      "Episode: 63500 224.0  Epsilon: 0.37  Mean Rewards 179.7\n",
      "Episode: 63600 304.0  Epsilon: 0.36  Mean Rewards 174.5\n",
      "Episode: 63700 422.0  Epsilon: 0.36  Mean Rewards 155.7\n",
      "Episode: 63800 267.0  Epsilon: 0.36  Mean Rewards 182.3\n",
      "Episode: 63900 201.0  Epsilon: 0.36  Mean Rewards 190.7\n",
      "Episode: 64000 143.0  Epsilon: 0.36  Mean Rewards 179.6\n",
      "Episode: 64100 303.0  Epsilon: 0.36  Mean Rewards 198.5\n",
      "Episode: 64200 54.0  Epsilon: 0.36  Mean Rewards 187.3\n",
      "Episode: 64300 264.0  Epsilon: 0.36  Mean Rewards 160.5\n",
      "Episode: 64400 231.0  Epsilon: 0.36  Mean Rewards 171.6\n",
      "Episode: 64500 339.0  Epsilon: 0.36  Mean Rewards 192.4\n",
      "Episode: 64600 231.0  Epsilon: 0.35  Mean Rewards 166.2\n",
      "Episode: 64700 336.0  Epsilon: 0.35  Mean Rewards 185.6\n",
      "Episode: 64800 401.0  Epsilon: 0.35  Mean Rewards 186.6\n",
      "Episode: 64900 52.0  Epsilon: 0.35  Mean Rewards 180.7\n",
      "Episode: 65000 203.0  Epsilon: 0.35  Mean Rewards 180.8\n",
      "Episode: 65100 260.0  Epsilon: 0.35  Mean Rewards 163.7\n",
      "Episode: 65200 73.0  Epsilon: 0.35  Mean Rewards 184.0\n",
      "Episode: 65300 139.0  Epsilon: 0.35  Mean Rewards 177.7\n",
      "Episode: 65400 213.0  Epsilon: 0.35  Mean Rewards 184.9\n",
      "Episode: 65500 220.0  Epsilon: 0.35  Mean Rewards 188.8\n",
      "Episode: 65600 147.0  Epsilon: 0.34  Mean Rewards 196.1\n",
      "Episode: 65700 401.0  Epsilon: 0.34  Mean Rewards 193.4\n",
      "Episode: 65800 213.0  Epsilon: 0.34  Mean Rewards 185.1\n",
      "Episode: 65900 87.0  Epsilon: 0.34  Mean Rewards 202.1\n",
      "Episode: 66000 226.0  Epsilon: 0.34  Mean Rewards 205.1\n",
      "Episode: 66100 405.0  Epsilon: 0.34  Mean Rewards 202.3\n",
      "Episode: 66200 23.0  Epsilon: 0.34  Mean Rewards 195.3\n",
      "Episode: 66300 250.0  Epsilon: 0.34  Mean Rewards 202.2\n",
      "Episode: 66400 139.0  Epsilon: 0.34  Mean Rewards 220.0\n",
      "Episode: 66500 159.0  Epsilon: 0.34  Mean Rewards 202.8\n",
      "Episode: 66600 268.0  Epsilon: 0.33  Mean Rewards 211.4\n",
      "Episode: 66700 214.0  Epsilon: 0.33  Mean Rewards 219.4\n",
      "Episode: 66800 34.0  Epsilon: 0.33  Mean Rewards 184.7\n",
      "Episode: 66900 41.0  Epsilon: 0.33  Mean Rewards 194.2\n",
      "Episode: 67000 31.0  Epsilon: 0.33  Mean Rewards 233.1\n",
      "Episode: 67100 99.0  Epsilon: 0.33  Mean Rewards 213.4\n",
      "Episode: 67200 158.0  Epsilon: 0.33  Mean Rewards 188.6\n",
      "Episode: 67300 106.0  Epsilon: 0.33  Mean Rewards 224.2\n",
      "Episode: 67400 433.0  Epsilon: 0.33  Mean Rewards 205.3\n",
      "Episode: 67500 161.0  Epsilon: 0.33  Mean Rewards 179.9\n",
      "Episode: 67600 193.0  Epsilon: 0.32  Mean Rewards 234.6\n",
      "Episode: 67700 269.0  Epsilon: 0.32  Mean Rewards 252.2\n",
      "Episode: 67800 158.0  Epsilon: 0.32  Mean Rewards 200.1\n",
      "Episode: 67900 134.0  Epsilon: 0.32  Mean Rewards 199.3\n",
      "Episode: 68000 329.0  Epsilon: 0.32  Mean Rewards 247.3\n",
      "Episode: 68100 290.0  Epsilon: 0.32  Mean Rewards 249.9\n",
      "Episode: 68200 188.0  Epsilon: 0.32  Mean Rewards 250.0\n",
      "Episode: 68300 146.0  Epsilon: 0.32  Mean Rewards 203.3\n",
      "Episode: 68400 101.0  Epsilon: 0.32  Mean Rewards 219.6\n",
      "Episode: 68500 275.0  Epsilon: 0.32  Mean Rewards 193.1\n",
      "Episode: 68600 236.0  Epsilon: 0.31  Mean Rewards 238.7\n",
      "Episode: 68700 251.0  Epsilon: 0.31  Mean Rewards 229.7\n",
      "Episode: 68800 226.0  Epsilon: 0.31  Mean Rewards 206.8\n",
      "Episode: 68900 369.0  Epsilon: 0.31  Mean Rewards 233.6\n",
      "Episode: 69000 353.0  Epsilon: 0.31  Mean Rewards 251.1\n",
      "Episode: 69100 148.0  Epsilon: 0.31  Mean Rewards 232.5\n",
      "Episode: 69200 81.0  Epsilon: 0.31  Mean Rewards 221.4\n",
      "Episode: 69300 165.0  Epsilon: 0.31  Mean Rewards 227.8\n",
      "Episode: 69400 131.0  Epsilon: 0.31  Mean Rewards 215.9\n",
      "Episode: 69500 131.0  Epsilon: 0.31  Mean Rewards 214.2\n",
      "Episode: 69600 126.0  Epsilon: 0.30  Mean Rewards 215.7\n",
      "Episode: 69700 292.0  Epsilon: 0.30  Mean Rewards 231.7\n",
      "Episode: 69800 259.0  Epsilon: 0.30  Mean Rewards 256.9\n",
      "Episode: 69900 217.0  Epsilon: 0.30  Mean Rewards 223.3\n",
      "Episode: 70000 209.0  Epsilon: 0.30  Mean Rewards 208.7\n",
      "Episode: 70100 64.0  Epsilon: 0.30  Mean Rewards 264.0\n",
      "Episode: 70200 149.0  Epsilon: 0.30  Mean Rewards 219.1\n",
      "Episode: 70300 172.0  Epsilon: 0.30  Mean Rewards 255.9\n",
      "Episode: 70400 617.0  Epsilon: 0.30  Mean Rewards 244.8\n",
      "Episode: 70500 237.0  Epsilon: 0.30  Mean Rewards 272.9\n",
      "Episode: 70600 159.0  Epsilon: 0.29  Mean Rewards 243.9\n",
      "Episode: 70700 216.0  Epsilon: 0.29  Mean Rewards 249.7\n",
      "Episode: 70800 473.0  Epsilon: 0.29  Mean Rewards 238.5\n",
      "Episode: 70900 308.0  Epsilon: 0.29  Mean Rewards 198.4\n",
      "Episode: 71000 712.0  Epsilon: 0.29  Mean Rewards 250.4\n",
      "Episode: 71100 141.0  Epsilon: 0.29  Mean Rewards 237.2\n",
      "Episode: 71200 277.0  Epsilon: 0.29  Mean Rewards 258.0\n",
      "Episode: 71300 305.0  Epsilon: 0.29  Mean Rewards 222.6\n",
      "Episode: 71400 134.0  Epsilon: 0.29  Mean Rewards 210.6\n",
      "Episode: 71500 237.0  Epsilon: 0.29  Mean Rewards 236.1\n",
      "Episode: 71600 435.0  Epsilon: 0.28  Mean Rewards 238.3\n",
      "Episode: 71700 363.0  Epsilon: 0.28  Mean Rewards 280.3\n",
      "Episode: 71800 245.0  Epsilon: 0.28  Mean Rewards 267.5\n",
      "Episode: 71900 397.0  Epsilon: 0.28  Mean Rewards 212.8\n",
      "Episode: 72000 496.0  Epsilon: 0.28  Mean Rewards 282.8\n",
      "Episode: 72100 133.0  Epsilon: 0.28  Mean Rewards 263.9\n",
      "Episode: 72200 504.0  Epsilon: 0.28  Mean Rewards 311.4\n",
      "Episode: 72300 458.0  Epsilon: 0.28  Mean Rewards 263.1\n",
      "Episode: 72400 91.0  Epsilon: 0.28  Mean Rewards 247.2\n",
      "Episode: 72500 192.0  Epsilon: 0.28  Mean Rewards 258.4\n",
      "Episode: 72600 219.0  Epsilon: 0.27  Mean Rewards 243.0\n",
      "Episode: 72700 469.0  Epsilon: 0.27  Mean Rewards 234.2\n",
      "Episode: 72800 43.0  Epsilon: 0.27  Mean Rewards 257.8\n",
      "Episode: 72900 539.0  Epsilon: 0.27  Mean Rewards 269.5\n",
      "Episode: 73000 41.0  Epsilon: 0.27  Mean Rewards 243.2\n",
      "Episode: 73100 183.0  Epsilon: 0.27  Mean Rewards 250.0\n",
      "Episode: 73200 155.0  Epsilon: 0.27  Mean Rewards 241.3\n",
      "Episode: 73300 143.0  Epsilon: 0.27  Mean Rewards 241.0\n",
      "Episode: 73400 167.0  Epsilon: 0.27  Mean Rewards 298.1\n",
      "Episode: 73500 58.0  Epsilon: 0.27  Mean Rewards 245.8\n",
      "Episode: 73600 193.0  Epsilon: 0.26  Mean Rewards 275.1\n",
      "Episode: 73700 62.0  Epsilon: 0.26  Mean Rewards 303.7\n",
      "Episode: 73800 134.0  Epsilon: 0.26  Mean Rewards 256.8\n",
      "Episode: 73900 593.0  Epsilon: 0.26  Mean Rewards 265.5\n",
      "Episode: 74000 130.0  Epsilon: 0.26  Mean Rewards 307.7\n",
      "Episode: 74100 318.0  Epsilon: 0.26  Mean Rewards 266.3\n",
      "Episode: 74200 499.0  Epsilon: 0.26  Mean Rewards 296.9\n",
      "Episode: 74300 288.0  Epsilon: 0.26  Mean Rewards 282.4\n",
      "Episode: 74400 176.0  Epsilon: 0.26  Mean Rewards 268.8\n",
      "Episode: 74500 229.0  Epsilon: 0.26  Mean Rewards 275.8\n",
      "Episode: 74600 288.0  Epsilon: 0.25  Mean Rewards 295.0\n",
      "Episode: 74700 39.0  Epsilon: 0.25  Mean Rewards 296.2\n",
      "Episode: 74800 551.0  Epsilon: 0.25  Mean Rewards 323.4\n",
      "Episode: 74900 253.0  Epsilon: 0.25  Mean Rewards 302.2\n",
      "Episode: 75000 387.0  Epsilon: 0.25  Mean Rewards 283.2\n",
      "Episode: 75100 144.0  Epsilon: 0.25  Mean Rewards 292.4\n",
      "Episode: 75200 521.0  Epsilon: 0.25  Mean Rewards 327.0\n",
      "Episode: 75300 614.0  Epsilon: 0.25  Mean Rewards 298.0\n",
      "Episode: 75400 334.0  Epsilon: 0.25  Mean Rewards 309.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 75500 223.0  Epsilon: 0.25  Mean Rewards 319.7\n",
      "Episode: 75600 83.0  Epsilon: 0.24  Mean Rewards 282.0\n",
      "Episode: 75700 395.0  Epsilon: 0.24  Mean Rewards 306.9\n",
      "Episode: 75800 317.0  Epsilon: 0.24  Mean Rewards 295.7\n",
      "Episode: 75900 435.0  Epsilon: 0.24  Mean Rewards 255.1\n",
      "Episode: 76000 449.0  Epsilon: 0.24  Mean Rewards 315.4\n",
      "Episode: 76100 492.0  Epsilon: 0.24  Mean Rewards 327.2\n",
      "Episode: 76200 269.0  Epsilon: 0.24  Mean Rewards 322.8\n",
      "Episode: 76300 239.0  Epsilon: 0.24  Mean Rewards 331.1\n",
      "Episode: 76400 273.0  Epsilon: 0.24  Mean Rewards 292.9\n",
      "Episode: 76500 308.0  Epsilon: 0.24  Mean Rewards 322.8\n",
      "Episode: 76600 155.0  Epsilon: 0.23  Mean Rewards 343.6\n",
      "Episode: 76700 750.0  Epsilon: 0.23  Mean Rewards 339.1\n",
      "Episode: 76800 915.0  Epsilon: 0.23  Mean Rewards 328.6\n",
      "Episode: 76900 156.0  Epsilon: 0.23  Mean Rewards 309.8\n",
      "Episode: 77000 23.0  Epsilon: 0.23  Mean Rewards 331.1\n",
      "Episode: 77100 214.0  Epsilon: 0.23  Mean Rewards 346.8\n",
      "Episode: 77200 296.0  Epsilon: 0.23  Mean Rewards 344.1\n",
      "Episode: 77300 536.0  Epsilon: 0.23  Mean Rewards 344.6\n",
      "Episode: 77400 213.0  Epsilon: 0.23  Mean Rewards 501.6\n",
      "Episode: 77500 285.0  Epsilon: 0.23  Mean Rewards 331.6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[136], line 105\u001b[0m\n\u001b[0;32m    102\u001b[0m     plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcartpole.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 105\u001b[0m     run(is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[136], line 58\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(is_training, render)\u001b[0m\n\u001b[0;32m     56\u001b[0m new_state_p \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdigitize(new_state[\u001b[38;5;241m0\u001b[39m], pos_space)\n\u001b[0;32m     57\u001b[0m new_state_v \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdigitize(new_state[\u001b[38;5;241m1\u001b[39m], vel_space)\n\u001b[1;32m---> 58\u001b[0m new_state_a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdigitize(new_state[\u001b[38;5;241m2\u001b[39m], ang_space)\n\u001b[0;32m     59\u001b[0m new_state_av\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdigitize(new_state[\u001b[38;5;241m3\u001b[39m], ang_vel_space)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_training:\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdigitize\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:5614\u001b[0m, in \u001b[0;36mdigitize\u001b[1;34m(x, bins, right)\u001b[0m\n\u001b[0;32m   5612\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bins) \u001b[38;5;241m-\u001b[39m _nx\u001b[38;5;241m.\u001b[39msearchsorted(bins[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], x, side\u001b[38;5;241m=\u001b[39mside)\n\u001b[0;32m   5613\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 5614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39msearchsorted(bins, x, side\u001b[38;5;241m=\u001b[39mside)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msearchsorted\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1413\u001b[0m, in \u001b[0;36msearchsorted\u001b[1;34m(a, v, side, sorter)\u001b[0m\n\u001b[0;32m   1345\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_searchsorted_dispatcher)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearchsorted\u001b[39m(a, v, side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, sorter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;124;03m    Find indices where elements should be inserted to maintain order.\u001b[39;00m\n\u001b[0;32m   1349\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1411\u001b[0m \n\u001b[0;32m   1412\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearchsorted\u001b[39m\u001b[38;5;124m'\u001b[39m, v, side\u001b[38;5;241m=\u001b[39mside, sorter\u001b[38;5;241m=\u001b[39msorter)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c313fe45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
